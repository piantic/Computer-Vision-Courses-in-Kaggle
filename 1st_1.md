- 데이터 전처리
    - 데이터 수집 (Collection)
    - 데이터 정제 (Clean Labels/Samples)
    - 데이터 증강 (Data Up/Down)
- AI 모델 개발(설계-평가-튜닝)
    - AI 모델 아키텍쳐 설계
    - XAI (Explainable AI)
    - AI 모델 학습 및 평가 (Learning curve 해석/검증)
    - AI 모델 튜닝 (HPO, Class Imbalanced Class)
- AI 시스템 구축(설계-배포-모니터링-자동화-최적화)
    - AI 시스템(ML PipeLine) 설계 및 배포
    - AI 시스템 모니터링 및 자동화 (지속운영)
    - AI 시스템 최적화
- 최신 트렌드
    - 최근 주요 AI 기술 발전 트렌드

- 학습자료
    - Full Stack Deep Learning
        - Spring2021
        - course-2022
    - Software with AI 방법론
        - 01 개발 프로세스
        - 03 DevMLOps
        - 04 인프라



문제: 합성곱 신경망 (CNN)은 어떤 종류의 데이터에 주로 사용되는가? 그리고 CNN의 주요 구성 요소는 무엇인가요?

문제: 순환 신경망 (RNN)은 어떤 유형의 데이터에 적합한가요? RNN의 핵심 개념인 장단기 메모리 (LSTM)는 왜 사용되는지 설명해보세요.

문제: 드롭아웃 (Dropout)과 배치 정규화 (Batch Normalization)은 과적합을 어떻게 방지하는데 도움이 되는가요? 각각의 작동 원리를 설명해주세요.

문제: 전이 학습 (Transfer Learning)은 어떤 상황에서 유용한가요? 전이 학습을 위해 사전 훈련된 모델을 어떻게 활용할 수 있는지 예를 들어 설명해보세요.

문제: 생성적 적대 신경망 (GAN)은 어떤 개념으로 구성되어 있으며, 어떤 목적으로 사용되는지 설명해주세요. GAN의 핵심 요소와 작동 원리를 간단히 설명해보세요.



정답: 합성곱 신경망 (CNN)은 주로 이미지 데이터에 사용됩니다. CNN은 이미지의 공간적인 구조를 고려하여 특징을 추출하고, 이를 통해 이미지 분류, 객체 감지, 세그멘테이션 등의 작업을 수행할 수 있습니다.

CNN의 주요 구성 요소는 다음과 같습니다:

합성곱 층 (Convolutional Layer): 입력 데이터에 필터를 적용하여 특징 맵을 생성합니다. 필터는 작은 윈도우 크기로 입력 데이터를 슬라이딩하면서 합성곱 연산을 수행합니다.
풀링 층 (Pooling Layer): 합성곱 층의 출력을 다운샘플링하여 공간적인 크기를 줄입니다. 풀링은 주로 최대값을 선택하는 맥스 풀링 (Max Pooling)이 사용되며, 특징 맵의 크기를 줄이고 계산량을 감소시킵니다.
활성화 함수 (Activation Function): 합성곱 층과 풀링 층 사이에 적용되며, 비선형성을 도입하여 신경망이 복잡한 패턴을 학습할 수 있게 합니다. ReLU (Rectified Linear Unit) 함수가 주로 사용됩니다.
완전 연결 층 (Fully Connected Layer): 특징 맵을 1차원 벡터로 변환하여 출력을 생성합니다. 이 층은 신경망의 마지막 부분이며, 분류나 회귀 등의 최종 작업을 수행합니다.



정답: 순환 신경망 (RNN)은 주로 순차적인 데이터, 시계열 데이터, 문장 등에 적용됩니다. RNN은 이전 단계의 출력을 현재 단계의 입력으로 사용하여 순차적인 의미를 학습하고 예측하는 데 효과적입니다.

RNN의 핵심 개념인 장단기 메모리 (LSTM)는 RNN의 장기 의존성 문제를 해결하기 위해 도입된 개선된 모델입니다. LSTM은 다음과 같은 구조로 이루어져 있습니다:

입력 게이트 (Input Gate): 현재 입력을 얼마나 기억할 것인지를 결정합니다.
삭제 게이트 (Forget Gate): 이전 메모리 상태에서 얼마나 많은 정보를 삭제할 것인지를 결정합니다.
출력 게이트 (Output Gate): 현재 입력과 이전 메모리 상태를 기반으로 새로운 메모리 상태를 생성합니다.
셀 상태 (Cell State): LSTM의 핵심 메모리로, 입력 게이트와 삭제 게이트에 의해 업데이트되며, 정보를 기억하고 전달합니다.
LSTM은 이전 시간 단계의 정보를 기억하는 능력과 기울기 소실 문제를 완화하는 능력을 가지고 있어, 시계열 데이터나 문장과 같은 순차적인 데이터의 의미를 잘 인식하고 예측할 수 있습니다.



정답: 드롭아웃 (Dropout)과 배치 정규화 (Batch Normalization)은 과적합을 방지하는 데 도움이 되는 기법입니다.

드롭아웃은 학습 과정에서 신경망의 일부 유닛을 임의로 제거하여 모델의 복잡성을 줄이는 방법입니다. 이를 통해 신경망이 특정 유닛에 과도하게 의존하는 것을 방지하고, 모든 유닛이 다양한 조합으로 작동하도록 합니다. 드롭아웃은 훈련 중에만 적용되며, 테스트나 예측 단계에서는 비활성화됩니다.

배치 정규화는 입력 데이터의 평균과 표준 편차를 사용하여 입력을 정규화하는 과정입니다. 이를 통해 입력 데이터의 분포를 안정화시키고, 학습 과정에서 그레디언트의 전파를 원활하게 합니다. 또한, 배치 정규화는 각 층의 활성화 함수 이전에 적용되며, 모델의 파라미터를 스케일과 이동하는 추가적인 파라미터를 이용하여 유연성을 높입니다.

드롭아웃과 배치 정규화는 모델의 일반화 성능을 향상시키고 과적합을 줄이는 효과를 가지고 있습니다. 드롭아웃은 신경망 내부에서 노이즈를 도입하여 앙상블 효과를 얻는 방식으로 과적합을 방지합니다. 배치 정규화는 데이터의 정규화를 통해 그레디언트의 안정성을 높여 학습을 빠르게 진행하고 과적합을 줄입니다.


배치 정규화의 입력 데이터 정규화 과정을 Train set, Validation set, Test set 관점에서 설명해드리겠습니다.

Train set: 훈련 데이터셋은 모델의 학습에 사용되는 데이터입니다. 배치 정규화는 각 훈련 배치에 대해 평균과 표준 편차를 계산하여 정규화를 수행합니다. 이 때, 훈련 데이터셋의 평균과 표준 편차를 사용하여 정규화를 진행합니다. 이러한 정규화는 학습 과정에서 그레디언트의 전파를 안정화시키고, 모델이 좀 더 일반화된 특징을 학습할 수 있도록 도와줍니다.

Validation set: 검증 데이터셋은 모델의 성능 평가를 위해 사용되는 데이터입니다. 배치 정규화에서는 훈련 과정에서 사용되는 배치의 평균과 표준 편차를 사용하여 정규화를 진행하므로, 검증 데이터셋에 대해서는 정규화를 직접 수행하지 않습니다. 대신, 훈련 데이터셋으로부터 계산된 평균과 표준 편차를 사용하여 정규화된 훈련 데이터셋을 기반으로 모델을 학습한 후, 검증 데이터셋을 사용하여 모델의 성능을 평가합니다.

Test set: 테스트 데이터셋은 모델의 최종 성능 평가를 위해 사용되는 데이터입니다. 마찬가지로, 배치 정규화에서는 훈련 데이터셋으로부터 계산된 평균과 표준 편차를 사용하여 테스트 데이터셋을 정규화합니다. 이는 훈련 데이터셋과 동일한 정규화 방식을 사용하여 모델의 성능을 일관성 있게 평가하기 위함입니다. 테스트 데이터셋에 대해서도 정규화를 적용하여 모델의 입력을 일관된 방식으로 처리합니다.

따라서, 배치 정규화에서는 Train set을 통해 평균과 표준 편차를 계산하고, Validation set과 Test set에는 훈련 데이터셋의 평균과 표준 편차를 사용하여 입력을 정규화합니다. 이를 통해 모델이 학습 데이터셋과 평가 데이터셋에서 일관된 방식으로 동작하도록 합니다.



정답: 전이 학습 (Transfer Learning)은 이미 학습된 모델의 지식을 다른 유사한 작업에 활용하는 기법입니다. 이는 기존에 큰 규모의 데이터셋과 계산 리소스를 사용하여 학습한 모델을 다른 작업에 재사용함으로써 작업에 필요한 데이터의 양과 학습에 소요되는 시간을 줄일 수 있습니다.

전이 학습은 주로 다음과 같은 방식으로 이루어집니다:

사전 학습된 모델 선택: 이미지 분류, 객체 감지 등 다양한 작업에 대해 사전 학습된 모델 (예: VGG, ResNet, Inception 등) 중 적절한 모델을 선택합니다. 이 모델은 대규모 데이터셋 (예: ImageNet)에서 사전에 학습된 상태로 제공됩니다.

모델의 일부 재사용: 전이 학습에서는 일반적으로 모델의 하위 층은 공통적인 시각적 특징을 학습한 특징 추출기로 사용되고, 상위 층은 특정 작업에 맞게 새로 학습됩니다. 따라서, 사전 학습된 모델에서 하위 층을 고정하고, 상위 층을 새로운 작업에 맞게 재학습합니다.

작은 데이터셋으로 재학습: 전이 학습에서는 작은 데이터셋으로 새로운 작업에 맞게 모델을 재학습합니다. 이는 새로운 작업의 특징을 모델에 빠르게 학습시키기 위한 방법입니다. 작은 데이터셋의 경우, 과적합 문제가 발생할 수 있으므로 데이터 증강 (Data Augmentation) 등의 기법을 활용하여 데이터를 확장시키는 것이 일반적입니다.

전이 학습은 초기부터 모델을 학습하는 것보다 적은 데이터와 시간으로 더 나은 성능을 달성할 수 있는 장점이 있습니다. 또한, 사전 학습된 모델의 일부를 재사용함으로써 일반화 능력이 향상되고, 데이터 부족 문제에 대한 대안이 될 수 있습니다.


정답: 하이퍼파라미터 튜닝은 AI 모델 아키텍처 설계 과정에서 모델의 성능을 향상시키기 위해 조정해야 하는 매개변수들을 조정하는 과정입니다. 이러한 하이퍼파라미터들은 모델의 학습 속도, 복잡도, 일반화 능력 등에 영향을 미칩니다.

하이퍼파라미터 튜닝은 주로 다음과 같은 방식으로 이루어집니다:

그리드 탐색 (Grid Search): 주어진 하이퍼파라미터 범위에서 가능한 모든 조합을 시도하여 최적의 조합을 찾는 방식입니다. 이 방법은 모든 조합을 시도하기 때문에 계산 비용이 크지만, 모든 조합을 공평하게 탐색하여 최적의 조합을 찾을 수 있습니다.

랜덤 탐색 (Random Search): 주어진 하이퍼파라미터 범위에서 임의로 선택된 조합을 시도하여 최적의 조합을 찾는 방식입니다. 그리드 탐색보다 계산 비용이 적게 들지만, 모든 조합을 공평하게 탐색하지 않기 때문에 최적의 조합을 보장하지는 않습니다.

베이즈 최적화 (Bayesian Optimization): 이전 시도 결과를 활용하여 다음 시도할 하이퍼파라미터 조합을 선택하는 방식입니다. 베이즈 최적화는 기존 시도 결과에 기반하여 확률 모델을 구축하고, 이를 활용하여 더 나은 조합을 찾는 데에 사용됩니다. 이 방법은 그리드 탐색과 랜덤 탐색보다 효율적으로 탐색할 수 있습니다.

하이퍼파라미터 튜닝은 모델의 성능을 최대화하기 위해 반복적으로 시도하고 평가하는 과정입니다. 이를 통해 최적의 하이퍼파라미터 조합을 찾아 모델의 성능을 향상시킬 수 있습니다.



문제 5번:
GAN의 아키텍처 설계에 사용되는 두 개의 주요 구성 요소는 무엇인지 설명하세요. 각 구성 요소의 역할에 대해서도 설명해주세요.

정답:
GAN의 아키텍처 설계에는 다음 두 가지 주요 구성 요소가 사용됩니다.

생성자 (Generator): 생성자는 실제 데이터와 유사한 가짜 데이터를 생성하는 역할을 합니다. 생성자는 주로 디코더로 구성되며, 랜덤 벡터 또는 노이즈로부터 시작하여, 학습을 통해 실제 데이터와 유사한 분포의 데이터를 생성하도록 훈련됩니다. 생성자는 노이즈를 입력으로 받아 실제 데이터와 유사한 출력을 생성함으로써 판별자를 속일 수 있도록 학습됩니다.

판별자 (Discriminator): 판별자는 생성된 가짜 데이터와 실제 데이터를 구별하는 역할을 합니다. 판별자는 일반적으로 이진 분류기로 구성되며, 생성자가 생성한 가짜 데이터와 실제 데이터를 구분하여 진짜/가짜를 판별하는 능력을 학습합니다. 판별자는 생성자의 출력과 실제 데이터를 입력으로 받아 이를 구분하는 확률 값을 출력합니다.

GAN은 생성자와 판별자 간의 경쟁과 학습 과정을 통해 점진적으로 성능을 개선하는 방식으로 작동합니다. 생성자는 판별자를 속이기 위해 더 실제 데이터와 유사한 가짜 데이터를 생성하려고 노력하고, 판별자는 생성자가 생성한 가짜 데이터와 실제 데이터를 정확하게 구별할 수 있도록 훈련됩니다. 이러한 경쟁과 학습 과정을 통해 생성자는 점점 실제 데이터와 더 유사한 데이터를 생성하게 되며, 판별자는 실제 데이터와 가짜 데이터를 더 정확하게 구분하게 됩니다.



Stable Diffusion과 GAN은 모두 이미지 생성 분야에서 사용되는 알고리즘 중 일부입니다. 하지만 두 방법은 다른 접근 방식과 특징을 가지고 있습니다.

GAN은 생성자와 판별자라는 두 개의 네트워크로 구성되어 있으며, 경쟁적인 학습을 통해 실제 데이터와 유사한 가짜 데이터를 생성합니다. GAN은 실제 데이터와 유사한 분포의 데이터를 생성할 수 있는데 주로 사용됩니다. 생성자와 판별자의 경쟁과정을 통해 높은 품질의 이미지를 생성할 수 있으며, 다양한 종류의 데이터 생성에도 적용될 수 있습니다.

반면 Stable Diffusion은 이미지 생성을 안정적으로 수행하는 방법 중 하나입니다. Diffusion 모델은 이미지의 픽셀 값을 점진적으로 업데이트하여 생성하는데, 안정적인 학습을 통해 이미지 품질을 향상시킬 수 있습니다. 또한 Diffusion 모델은 연속적인 과정으로 데이터를 생성하기 때문에 생성된 데이터의 특성을 조절할 수 있는 장점이 있습니다.

따라서, GAN은 주로 이미지 생성에 사용되는 반면 Stable Diffusion은 안정적인 학습과 데이터 특성의 조절 가능성에 중점을 둔다는 차이가 있습니다. 하지만 Stable Diffusion 역시 이미지 생성 분야에서 사용되는 경우가 많습니다. 선택하는 방법은 사용하는 데이터, 목표 및 요구 사항에 따라 다를 수 있습니다.